{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch \n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = \"C:/Users/hasee/Desktop/Germany_2024/Dataset/NWRD/val\"\n",
    "dest = \"C:/Users/hasee/Desktop/Germany_2024/Dataset/datasetTesting\"\n",
    "patch_size = 224\n",
    "rust_threshold = 150\n",
    "max_number_of_images_per_group = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This snippet will make patches of the images in the destination/patches directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[WinError 183] Cannot create a file when that file already exists: 'C:/Users/hasee/Desktop/Germany_2024/Dataset/datasetTesting\\\\patches'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m images_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(patches_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m masks_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(patches_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmasks\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatches_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(images_dir)\n\u001b[0;32m     11\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(masks_dir)\n",
      "File \u001b[1;32m<frozen os>:225\u001b[0m, in \u001b[0;36mmakedirs\u001b[1;34m(name, mode, exist_ok)\u001b[0m\n",
      "\u001b[1;31mFileExistsError\u001b[0m: [WinError 183] Cannot create a file when that file already exists: 'C:/Users/hasee/Desktop/Germany_2024/Dataset/datasetTesting\\\\patches'"
     ]
    }
   ],
   "source": [
    "masks_paths = glob.glob(f'{os.path.join(source, \"masks\", \"*\")}')\n",
    "images_paths = glob.glob(f'{os.path.join(source, \"images\", \"*\")}')\n",
    "images_paths.sort()\n",
    "masks_paths.sort()\n",
    "\n",
    "patches_path = os.path.join(dest, \"patches\")\n",
    "images_dir = os.path.join(patches_path, \"images\")\n",
    "masks_dir = os.path.join(patches_path, \"masks\")\n",
    "os.makedirs(patches_path)\n",
    "os.makedirs(images_dir)\n",
    "os.makedirs(masks_dir)\n",
    "\n",
    "def create_patches(fname):\n",
    "    x = 0\n",
    "    y = 0\n",
    "    patches = []\n",
    "    img = cv2.imread(fname)\n",
    "    print(\"image shape:\",img.shape)\n",
    "    p_num = 0\n",
    "    while (y + patch_size < img.shape[0]):\n",
    "        \n",
    "        if (x + patch_size > img.shape[1]):\n",
    "            x = 0\n",
    "            y += patch_size\n",
    "        if y + patch_size <= img.shape[0] and x + patch_size <= img.shape[1]:\n",
    "            patches.append([x, y])\n",
    "        x += patch_size\n",
    "    print(\"total patches: \", len(patches))\n",
    "    return patches\n",
    "\n",
    "total_count = 0\n",
    "for u in images_paths:\n",
    "    print(u)\n",
    "    patches = create_patches(u)\n",
    "    bgr = cv2.imread(u)\n",
    "    image_name = u.split('/')[-1].split('.')[0]\n",
    "    total_count += len(patches)\n",
    "\n",
    "    for count, P in enumerate(patches):\n",
    "        cv2.imwrite(os.path.join(images_dir,f\"{image_name}_{count}.png\"), bgr[P[1]:P[1]+patch_size,P[0]:P[0]+patch_size])\n",
    "            \n",
    "print(\"total image count:\", total_count)\n",
    "\n",
    "total_count = 0\n",
    "for u in masks_paths:\n",
    "    print(u)\n",
    "    patches = create_patches(u)\n",
    "    bgr = cv2.imread(u)\n",
    "    image_name = u.split('/')[-1].split('.')[0]\n",
    "\n",
    "    total_count += len(patches)\n",
    "\n",
    "    for count, P in enumerate(patches):\n",
    "        cv2.imwrite(os.path.join(masks_dir,f\"{image_name}_{count}.png\"), bgr[P[1]:P[1]+patch_size,P[0]:P[0]+patch_size])\n",
    "            \n",
    "print(\"total masks count:\", total_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will saperate the rust and non rust patches and put them in and put them in directory destination/RustNonRustSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum rust patch: 0\n",
      "minimum rust patch white pixels: 1000\n",
      "rust count= 0\n",
      "non rust count= 0\n"
     ]
    }
   ],
   "source": [
    "destination = os.path.join(dest, \"RustNonRustSplit\")\n",
    "root = patches_path\n",
    "\n",
    "os.makedirs(destination)\n",
    "os.makedirs(os.path.join(destination,\"non_rust\",\"images\"))\n",
    "os.makedirs(os.path.join(destination,\"non_rust\",\"masks\"))\n",
    "os.makedirs(os.path.join(destination,\"rust\",\"images\"))\n",
    "os.makedirs(os.path.join(destination,\"rust\",\"masks\"))\n",
    "\n",
    "masks_path = os.path.join(root, \"masks\", \"*.png\")\n",
    "masks_paths = glob.glob(masks_path)\n",
    "minimum=1000\n",
    "min_patch=0\n",
    "rust_count=0\n",
    "non_rust_count=0\n",
    "\n",
    "for mask_path in masks_paths:\n",
    "    patch_name = mask_path.split(\"/\")[-1].split(\".\")[0]\n",
    "    \n",
    "    patch_mask = cv2.imread(mask_path, 0)\n",
    "    patch_img = cv2.imread(os.path.join(root, \"images\",patch_name+\".png\"))\n",
    "\n",
    "    condition = (patch_mask > 150)\n",
    "    count = np.sum(condition)\n",
    "        \n",
    "    if count<=rust_threshold:\n",
    "            cv2.imwrite(os.path.join(destination,\"non_rust\",\"images\",f\"{patch_name}.png\"), patch_img)\n",
    "            cv2.imwrite(os.path.join(destination,\"non_rust\",\"masks\",f\"{patch_name}.png\"), patch_mask)\n",
    "            non_rust_count+=1\n",
    "    else:\n",
    "            if (count<=minimum):\n",
    "                   minimum=count\n",
    "                   min_patch = patch_name\n",
    "            cv2.imwrite(os.path.join(destination,\"rust\",\"images\",f\"{patch_name}.png\"), patch_img)\n",
    "            cv2.imwrite(os.path.join(destination,\"rust\",\"masks\",f\"{patch_name}.png\"), patch_mask)\n",
    "            rust_count+=1\n",
    "\n",
    "print(\"minimum rust patch:\",min_patch)\n",
    "print(\"minimum rust patch white pixels:\",minimum)\n",
    "print(\"rust count=\", rust_count)\n",
    "print(\"non rust count=\", non_rust_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next two code snippets for training only. The following code will augment the images in the destination/RustNonRustSplit/images and destination/RustNonRustSplit/masks folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flip images horizontally\n",
    "def flip_images_hor(input_image):\n",
    "    # Iterate over the images in the input directory\n",
    "    transform_hflip = T.RandomHorizontalFlip(p=1.0)  # Set probability to 1.0 to always flip\n",
    "    return transform_hflip(input_image)\n",
    "\n",
    "#flip images vertically\n",
    "def flip_images_ver(input_image):\n",
    "    # Iterate over the images in the input directory\n",
    "    transform_vflip = T.RandomVerticalFlip(p=1.0)  # Set probability to 1.0 to always flip\n",
    "    return transform_vflip(input_image) \n",
    "  \n",
    "def shear_vertical(input_image, shear_factor=45):\n",
    "    # Apply vertical shear\n",
    "    sheared_image = TF.affine(input_image, angle=0, translate=(0, 0), scale=1, shear=(0, shear_factor))\n",
    "    return sheared_image\n",
    "\n",
    "def shear_horizontal(input_image, shear_factor=45):  # Increased shear for testing\n",
    "    sheared_image = TF.affine(input_image, angle=0, translate=(0, 0), scale=1, shear=(shear_factor, 0))\n",
    "    return sheared_image\n",
    "\n",
    "def rotate_images(input_image, angle=45):\n",
    "    # Convert PIL Image to NumPy array\n",
    "    input_array = np.array(input_image)\n",
    "    # Rotate the image\n",
    "    height, width = input_array.shape[:2]\n",
    "    rotation_matrix = cv2.getRotationMatrix2D((width / 2, height / 2), angle, 1)\n",
    "    rotated_array = cv2.warpAffine(input_array, rotation_matrix, (width, height))\n",
    "    # Convert NumPy array back to PIL Image\n",
    "    rotated_image = Image.fromarray(rotated_array)\n",
    "    return rotated_image\n",
    "\n",
    "def dark(input_image,gamma):\n",
    "    dark_image= TF.adjust_gamma(input_image, gamma)\n",
    "    return dark_image\n",
    "\n",
    "def augment_image(img_path):\n",
    "\n",
    "  # Apply the transformations\n",
    "  \n",
    "    #orig_image\n",
    "    orig_img = Image.open(Path(img_path))\n",
    "    \n",
    "    #flip images\n",
    "    img_hflipped = flip_images_hor(orig_img)\n",
    "    img_vflipped = flip_images_ver(orig_img)\n",
    "    \n",
    "    \n",
    "    #shear images\n",
    "    hor_shear = shear_horizontal(orig_img)\n",
    "    ver_shear = shear_vertical(orig_img)\n",
    "    \n",
    "    #dark\n",
    "    img_dark = dark(img_hflipped, 2)\n",
    "    img_rot = rotate_images(orig_img, angle=45)\n",
    "    \n",
    "    return [img_dark,img_hflipped,img_vflipped,hor_shear,ver_shear, img_rot]\n",
    "\n",
    "def creating_file_with_augmented_images(file_path_master_dataset, file_path_augmented_images):\n",
    "    master_dataset_folder = file_path_master_dataset\n",
    "    files_in_master_dataset = os.listdir(file_path_master_dataset)\n",
    "    augmented_images_folder = file_path_augmented_images\n",
    "    \n",
    "    for image_name in files_in_master_dataset:\n",
    "        image_path = os.path.join(master_dataset_folder, image_name)\n",
    "        required_images = augment_image(image_path)  # Assuming augment_image is defined elsewhere\n",
    "        i = 0\n",
    "        for augmented_image in required_images:\n",
    "            # Convert RGBA to RGB if necessary\n",
    "            if augmented_image.mode == 'RGBA':\n",
    "                augmented_image = augmented_image.convert('RGB')\n",
    "                \n",
    "            # Save as png\n",
    "            augmented_image_path = os.path.join(augmented_images_folder, f\"aug{i}_{image_name}\")\n",
    "            augmented_image.save(augmented_image_path, format='png')\n",
    "            i += 1\n",
    "\n",
    "master_dataset = os.path.join(destination,\"rust\",\"images\")\n",
    "augmented_dataset = os.path.join(destination,\"rust\",\"images\")\n",
    "creating_file_with_augmented_images(master_dataset,augmented_dataset)\n",
    "\n",
    "master_dataset = os.path.join(destination,\"rust\",\"masks\")\n",
    "augmented_dataset = os.path.join(destination,\"rust\",\"masks\")\n",
    "creating_file_with_augmented_images(master_dataset,augmented_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run next snippet only for training dataset. To remove patches that have their rust removed becuase of their augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum rust patch: 0\n",
      "minimum rust patch white pixels: 1000\n",
      "rust count= 0\n",
      "non rust count= 0\n"
     ]
    }
   ],
   "source": [
    "root = os.path.join(destination,\"rust\")\n",
    "\n",
    "non_rust_images_dir = os.path.join(destination,\"non_rust\",\"images\")\n",
    "non_rust_masks_dir = os.path.join(destination,\"non_rust\",\"masks\")\n",
    "\n",
    "masks_path = os.path.join(root, \"masks\", \"*.png\")\n",
    "masks_paths = glob.glob(masks_path)\n",
    "minimum=1000\n",
    "min_patch=0\n",
    "rust_count=0\n",
    "non_rust_count=0\n",
    "\n",
    "for mask_path in masks_paths:\n",
    "    patch_name = mask_path.split(\"/\")[-1].split(\".\")[0]\n",
    "    \n",
    "    patch_mask = cv2.imread(mask_path, 0)\n",
    "    patch_img = cv2.imread(os.path.join(root, \"images\",patch_name+\".png\"))\n",
    "\n",
    "    condition = (patch_mask > 150)\n",
    "    count = np.sum(condition)\n",
    "        \n",
    "    if count<=rust_threshold:\n",
    "            os.remove(mask_path)\n",
    "            os.remove(os.path.join(root, \"images\",patch_name+\".png\"))\n",
    "\n",
    "            cv2.imwrite(os.path.join(non_rust_images_dir,f\"{patch_name}.png\"), patch_img)\n",
    "            cv2.imwrite(os.path.join(non_rust_masks_dir,f\"{patch_name}.png\"), patch_mask)\n",
    "            non_rust_count+=1\n",
    "    else:\n",
    "            if (count<=minimum):\n",
    "                   minimum=count\n",
    "                   min_patch = patch_name\n",
    "        #     cv2.imwrite(os.path.join(destination,\"rust\",\"images\",f\"{patch_name}.png\"), patch_img)\n",
    "        #     cv2.imwrite(os.path.join(destination,\"rust\",\"masks\",f\"{patch_name}.png\"), patch_mask)\n",
    "            rust_count+=1\n",
    "\n",
    "print(\"minimum rust patch:\",min_patch)\n",
    "print(\"minimum rust patch white pixels:\",minimum)\n",
    "print(\"rust count=\", rust_count)\n",
    "print(\"non rust count=\", non_rust_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataset for classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/Users/hasee/Desktop/Germany_2024/Dataset/datasetTesting\\\\calssification\\\\non_rust'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rust_images_dir = os.path.join(destination,\"rust\",\"images\")\n",
    "non_rust_images_dir = os.path.join(destination,\"non_rust\",\"images\")\n",
    "\n",
    "rustClassificationDir = os.path.join(dest, \"calssification\", \"rust\")\n",
    "nonRustClassificationDir = os.path.join(dest, \"calssification\", \"non_rust\")\n",
    "os.makedirs(rustClassificationDir, exist_ok=True)\n",
    "os.makedirs(nonRustClassificationDir, exist_ok=True)\n",
    "\n",
    "shutil.copytree(rust_images_dir,rustClassificationDir, dirs_exist_ok=True)\n",
    "shutil.copytree(non_rust_images_dir,nonRustClassificationDir, dirs_exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next code snippet for training dataset only. It deletes non-rust patches to match rust patches in the classification folder only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def delete_extra_images(directory, target_count):\n",
    "    # Get a list of all image files in the directory\n",
    "    image_files = glob.glob(os.path.join(directory, '*.JPG')) + glob.glob(os.path.join(directory, '*.jpeg')) + glob.glob(os.path.join(directory, '*.png'))\n",
    "    \n",
    "    # Check if the number of images exceeds the target count\n",
    "    if len(image_files) > target_count:\n",
    "        # Calculate the number of images to delete\n",
    "        num_to_delete = len(image_files) - target_count\n",
    "        # Sort the images by modification time (oldest first)\n",
    "        image_files.sort(key=os.path.getmtime)\n",
    "        # Delete the extra images\n",
    "        for i in range(num_to_delete):\n",
    "            os.remove(image_files[i])\n",
    "        print(f\"{num_to_delete} images deleted.\")\n",
    "    elif len(image_files) < target_count:\n",
    "        print(\"Warning: Number of images in directory is less than the target count.\")\n",
    "\n",
    "if len(os.listdir(rustClassificationDir))< len(os.listdir(nonRustClassificationDir)):\n",
    "    delete_extra_images(nonRustClassificationDir, len(os.listdir(rustClassificationDir)))\n",
    "else:\n",
    "    delete_extra_images(rustClassificationDir, len(os.listdir(nonRustClassificationDir)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code creates a coslaiency style structure for co-saliency models training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rust_dir = os.path.join(destination,\"rust\")\n",
    "rustCosaliencynDir = os.path.join(dest, \"cosaliency\")\n",
    "shutil.copytree(rust_dir,rustCosaliencynDir, dirs_exist_ok=True)\n",
    "\n",
    "# Function to split images into folders based on image number\n",
    "def split_images_into_folders(source_dir, destination_dir):\n",
    "    # Create destination directory if it doesn't exist\n",
    "    if not os.path.exists(destination_dir):\n",
    "        os.makedirs(destination_dir)\n",
    "    # Iterate through files in the source directory\n",
    "    for filename in os.listdir(source_dir):\n",
    "        if filename.endswith('.png'):\n",
    "            image_no = filename.split('_')[0]  # Extract image number from filename\n",
    "            if not image_no.isdigit():\n",
    "                image_no = filename.split('_')[1]\n",
    "            destination_subdir = os.path.join(destination_dir, image_no)\n",
    "            # Create subdirectory if it doesn't exist\n",
    "            if not os.path.exists(destination_subdir):\n",
    "                os.makedirs(destination_subdir)\n",
    "            # Move the image file to the respective subdirectory\n",
    "            shutil.move(os.path.join(source_dir, filename), destination_subdir)\n",
    "\n",
    "def organize_images(main_directory):\n",
    "    # Ensure the main directory exists\n",
    "    if not os.path.exists(main_directory):\n",
    "        print(f\"The specified main directory '{main_directory}' does not exist.\")\n",
    "        return\n",
    "\n",
    "    # Get a list of subdirectories in the main directory\n",
    "    subdirectories = [d for d in os.listdir(main_directory) if os.path.isdir(os.path.join(main_directory, d))]\n",
    "\n",
    "    # Process each subdirectory\n",
    "    for subdir in subdirectories:\n",
    "        subdir_path = os.path.join(main_directory, subdir)\n",
    "\n",
    "        # Get a list of images in the subdirectory\n",
    "        images = [f for f in os.listdir(subdir_path) if f.lower().endswith(('.jpg', '.jpeg', '.png', '.gif'))]\n",
    "        # Determine the number of images per subdirectory\n",
    "        images_per_subdir = 12\n",
    "        num_subdirectories = len(images) // images_per_subdir\n",
    "        n=0\n",
    "        # Create additional subdirectories if needed\n",
    "        for i in range(num_subdirectories - 1):\n",
    "            new_subdir_name = f\"{subdir}_part{i + 1}\"\n",
    "            new_subdir_path = os.path.join(main_directory, new_subdir_name)\n",
    "\n",
    "            # Create the new subdirectory\n",
    "            os.makedirs(new_subdir_path)\n",
    "\n",
    "            # Move images to the new subdirectory\n",
    "            for j in range(images_per_subdir):\n",
    "                old_image_path = os.path.join(subdir_path, images[n])\n",
    "                new_image_path = os.path.join(new_subdir_path, images[n])\n",
    "                shutil.move(old_image_path, new_image_path)\n",
    "                n+=1\n",
    "\n",
    "source_directory = os.path.join(dest, \"cosaliency\", \"images\")\n",
    "destination_directory = os.path.join(dest, \"cosaliency\", \"images\")\n",
    "split_images_into_folders(source_directory, destination_directory)\n",
    "organize_images(destination_directory)\n",
    "\n",
    "source_directory = os.path.join(dest, \"cosaliency\", \"masks\")\n",
    "destination_directory = os.path.join(dest, \"cosaliency\", \"masks\")\n",
    "split_images_into_folders(source_directory, destination_directory)\n",
    "organize_images(destination_directory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "segformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

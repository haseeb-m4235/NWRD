{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"vbmO6ypJNSa4"},"outputs":[],"source":["import torch\n","from torch.utils.data import DataLoader\n","from transformers import AdamW, ViTImageProcessor, ViTForImageClassification\n","from NWRD_dataset import NWRD\n","from tqdm import tqdm\n","import numpy as np\n","import torch.nn.functional as F\n","import os\n","from PIL import Image\n","from torchvision.utils import save_image\n","from torchvision import transforms\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SCGTwtwrNSa5","outputId":"3d560402-6574-4d0b-83e1-fafd52ea6a25"},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]}],"source":["device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","CUDA_LAUNCH_BLOCKING=1\n","TORCH_USE_CUDA_DSA=1\n","print(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KXkIe66-NSa6"},"outputs":[],"source":["transformations = transforms.Compose([\n","    transforms.ToTensor()            # Convert the image to a PyTorch tensor\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PvFwLbsXNSa6"},"outputs":[],"source":["#test_ds = NWRD(root_dir=\"/media/tukl/2dceb8df-2a02-4725-92fb-525b406c4482/Hirra/DATASETS/NWRD-classifier/test\", train=False, transform=transformations)\n","#test_ds = NWRD(root_dir=\"/home/Hirra/DATASETS/NWRD-classifier/test\", train=False, transform=transformations)\n","test_ds = NWRD(root_dir=\"/home/Hirra/DATASETS/NWRD_FINAL_Filtered_Masks_threshold9_classifier\", train=False, transform=transformations)\n","\n","\n","test_loader = DataLoader(test_ds, batch_size=1, shuffle=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9xdmY6RANSa7","outputId":"917ae501-b9c1-4960-a892-edf7116a91cc"},"outputs":[{"data":{"text/plain":["ViTForImageClassification(\n","  (vit): ViTModel(\n","    (embeddings): ViTEmbeddings(\n","      (patch_embeddings): ViTPatchEmbeddings(\n","        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n","      )\n","      (dropout): Dropout(p=0.0, inplace=False)\n","    )\n","    (encoder): ViTEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x ViTLayer(\n","          (attention): ViTAttention(\n","            (attention): ViTSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.0, inplace=False)\n","            )\n","            (output): ViTSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.0, inplace=False)\n","            )\n","          )\n","          (intermediate): ViTIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ViTOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","      )\n","    )\n","    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","  )\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",")"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n","model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n","model.classifier = torch.nn.Linear(768,2)\n","model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mO3lKIdCNSa8","outputId":"2b8d3468-415d-4cb8-ffec-fc01ea24dc75"},"outputs":[{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["\n","#model_weights = torch.load('/home/Hirra/coding_files/crossvit/weights/wandb_vit_base_final_hard_val_NWRD_epoch_50_lr_0.000000001_wd_0.001_batch_size_8_unaugmented_unequlaized/49.pth') #use 33.pth the lowest\n","model_weights = torch.load('/home/Hirra/coding_files/crossvit/weights/wandb_vit_base_final_val_NWRD_epoch_50_lr_0.000000003_wd_0.001_batch_size_8_unaugmented_unequlaized/28.pth') #use 33.pth the lowest\n","model.load_state_dict(model_weights.state_dict())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ehaI80-iNSa8"},"outputs":[],"source":["criterion = torch.nn.CrossEntropyLoss()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AekRMtUbNSa9","outputId":"fa46b01a-744c-406e-dd21-f17bd3344511"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 3508/3508 [00:44<00:00, 79.62it/s]"]},{"name":"stdout","output_type":"stream","text":["True Positives: 551\n","False Positives: 232\n","True Negatives: 2619\n","False Negatives: 106\n","Precision: 0.7037037037036138\n","Recall: 0.8386605783864781\n","F1: 0.7652777777776716\n","Accuracy: 0.9036488027365763\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["#Testing\n","\n","true_positives = 0\n","false_positives = 0\n","true_negatives = 0\n","false_negatives = 0\n","#import torch.nn as nn\n","#import torch.nn.functional as F\n","# rust_dir = \"/home/Hirra/coding_files/crossvit/results/wandb_vit_base_NWRD_epoch_50_lr_0.000000003_wd_0.001_batch_size_8_augmented_equlaized/rust\"\n","# fn_dir = \"/home/Hirra/coding_files/crossvit/results/wandb_vit_base_NWRD_epoch_50_lr_0.000000003_wd_0.001_batch_size_8_augmented_equlaized/fn\"\n","# fp_dir = \"/home/Hirra/coding_files/crossvit/results/wandb_vit_base_NWRD_epoch_50_lr_0.000000003_wd_0.001_batch_size_8_augmented_equlaized/fp\"\n","\n","rust_dir = \"/home/Hirra/coding_files/crossvit/results/wandb_vit_base_updated_val_NWRD_epoch_50_lr_0.000000003_wd_0.001_batch_size_8_unaugmented_unequlaized/rust\"\n","fn_dir = \"/home/Hirra/coding_files/crossvit/results/wandb_vit_base_updated_val_NWRD_epoch_50_lr_0.000000003_wd_0.001_batch_size_8_unaugmented_unequlaized/fn\"\n","fp_dir = \"/home/Hirra/coding_files/crossvit/results/wandb_vit_base_updated_val_NWRD_epoch_50_lr_0.000000003_wd_0.001_batch_size_8_unaugmented_unequlaized/fp\"\n","\n","img_paths = test_ds.images\n","count=0\n","#softmax = nn.SoftMax()\n","model.eval\n","loop = tqdm(enumerate(test_loader), total=len(test_loader))\n","with torch.no_grad():\n","    for batch_idx, (images, labels) in loop:\n","        inputs = processor(images=images, return_tensors=\"pt\", do_rescale=False).to(device)\n","        #print(inputs)\n","        labels = labels.to(device)\n","\n","        outputs = model(**inputs)\n","        logits = outputs.logits\n","        #logits = F.softmax(logits)\n","        prediction = logits.argmax(axis=1)\n","        # print(logits)\n","        # print(labels)\n","        # print(prediction)\n","        if (prediction==1):\n","            image_path = os.path.join(rust_dir, img_paths[count].split('/')[-1])\n","            image = images.squeeze().cpu()\n","            #save_image(image,image_path)\n","\n","        if ((prediction==1) and (labels==0)):\n","            image_path = os.path.join(fp_dir, img_paths[count].split('/')[-1])\n","            image = images.squeeze().cpu()\n","            #save_image(image,image_path)\n","\n","        if ((prediction==0) and (labels==1)):\n","            image_path = os.path.join(fn_dir, img_paths[count].split('/')[-1])\n","            image = images.squeeze().cpu()\n","            #save_image(image,image_path)\n","\n","        true_positives += torch.sum((prediction == 1) & (labels == 1)).item()\n","        false_positives += torch.sum((prediction == 1) & (labels == 0)).item()\n","        true_negatives += torch.sum((prediction == 0) & (labels == 0)).item()\n","        false_negatives += torch.sum((prediction == 0) & (labels == 1)).item()\n","        count+=1\n","\n","# Calculate metrics\n","precision = true_positives / (true_positives + false_positives + 1e-10)\n","recall = true_positives / (true_positives + false_negatives + 1e-10)\n","F1 = 2 * (precision * recall) / (precision + recall)\n","accuracy = (true_positives + true_negatives) / (true_positives + false_positives + true_negatives + false_negatives + 1e-10)\n","\n","# Print or use the metrics as needed\n","print(f\"True Positives: {true_positives}\")\n","print(f\"False Positives: {false_positives}\")\n","print(f\"True Negatives: {true_negatives}\")\n","print(f\"False Negatives: {false_negatives}\")\n","print(f\"Precision: {precision}\")\n","print(f\"Recall: {recall}\")\n","print(f\"F1: {F1}\")\n","print(f\"Accuracy: {accuracy}\")"]},{"cell_type":"markdown","metadata":{"id":"lt3-dNUlNSa-"},"source":[]}],"metadata":{"kernelspec":{"display_name":"crossvit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}